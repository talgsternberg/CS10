{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf600
{\fonttbl\f0\fnil\fcharset0 HelveticaNeue-Bold;\f1\fnil\fcharset0 HelveticaNeue;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\deftab560
\pard\pardeftab560\slleading20\partightenfactor0

\f0\b\fs26 \cf0 **Note: we are using our free extension for PSET5**         :)\

\f1\b0\fs24 \
Our hard-coded ice cream graph is correct as in it gives us the same output as what we would expect (ie: two cones, three cones, two cones is correlated to hot, hot, hot). We traced backtrack in the ice-cream data given in class and this is the answer we expect.\
\
\
Running Viterbi on PS-5/simple-test-sentences.txt trained with PS-5/simple-train-tags.txt and PS-5/simple-train-sentences.txt:\
got 32 tags correct and 5 tags incorrect.\
\
\
Running Viterbi on PS-5/brown-test-sentences.txt trained with PS-5/brown-train-tags.txt and PS-5/brown-train-sentences.txt:\
 got 35108 tags correct and 1286 tags incorrect.\
\
\
Overall, our testing turned out to be pretty accurate. Unseen word penalty was -100. Because the penalty is in a correct range(not too high/too low), we end up predicting things correctly as opposed to rejecting or accepting values that do not make sense. \
\
\
Examples of sentences that are tagged as expected: \'93my watch glows in the night.\'94, \'93I walked your dog for money.\'94\
\
Examples of sentences that are not tagged as expected: \'93the dog saw trains in the night.\'94, \'93my dog trains to bark.\'94\
\pard\pardeftab560\slleading20\pardirnatural\partightenfactor0
\cf0 \
\
}